{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering\n",
    "## Real Estate Price Prediction\n",
    "\n",
    "**Author:** Nicolas  \n",
    "**Date:** 2025-01-09  \n",
    "**Objective:** Cr√©er et transformer les features pour optimiser la performance des mod√®les ML\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "1. [Data Loading](#1-data-loading)\n",
    "2. [Outlier Treatment](#2-outlier-treatment)\n",
    "3. [Feature Creation](#3-feature-creation)\n",
    "4. [Encoding Categorical Variables](#4-encoding-categorical-variables)\n",
    "5. [Feature Scaling](#5-feature-scaling)\n",
    "6. [Target Transformation](#6-target-transformation)\n",
    "7. [Feature Selection](#7-feature-selection)\n",
    "8. [Train/Test Split](#8-train-test-split)\n",
    "9. [Export Processed Data](#9-export-processed-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "df = pd.read_csv('../data/real_estate_data.csv')\n",
    "\n",
    "print(f\"Dataset charg√©: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour traiter les outliers\n",
    "def cap_outliers_iqr(df, column, factor=1.5):\n",
    "    \"\"\"\n",
    "    Cap les outliers en utilisant la m√©thode IQR.\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    \n",
    "    # Cap values\n",
    "    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return df, lower_bound, upper_bound\n",
    "\n",
    "# Traiter les outliers pour les variables cl√©s\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAITEMENT DES OUTLIERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "outlier_cols = ['price', 'surface_m2', 'age_years']\n",
    "\n",
    "for col in outlier_cols:\n",
    "    before = df[col].describe()\n",
    "    df, lower, upper = cap_outliers_iqr(df, col)\n",
    "    after = df[col].describe()\n",
    "    \n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(f\"  Bornes: [{lower:.2f}, {upper:.2f}]\")\n",
    "    print(f\"  Avant: min={before['min']:.2f}, max={before['max']:.2f}\")\n",
    "    print(f\"  Apr√®s: min={after['min']:.2f}, max={after['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CR√âATION DE NOUVELLES FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Prix au m¬≤\n",
    "df['price_per_m2'] = df['price'] / df['surface_m2']\n",
    "\n",
    "# 2. Surface par pi√®ce\n",
    "df['surface_per_room'] = df['surface_m2'] / df['rooms']\n",
    "\n",
    "# 3. Age bins (cat√©gories)\n",
    "df['age_category'] = pd.cut(df['age_years'], \n",
    "                            bins=[-1, 5, 15, 30, 100],\n",
    "                            labels=['Neuf', 'R√©cent', 'Moyen', 'Ancien'])\n",
    "\n",
    "# 4. Surface bins\n",
    "df['surface_category'] = pd.cut(df['surface_m2'],\n",
    "                                bins=[0, 40, 70, 100, 1000],\n",
    "                                labels=['Studio', 'Moyen', 'Grand', 'Tr√®s_grand'])\n",
    "\n",
    "# 5. Score de confort (combinaison de features)\n",
    "df['comfort_score'] = (df['has_elevator'].astype(int) + \n",
    "                       df['has_parking'].astype(int) + \n",
    "                       df['has_balcony'].astype(int))\n",
    "\n",
    "# 6. √âtage cat√©goriel\n",
    "df['floor_category'] = df['floor'].apply(lambda x: 'RDC' if x == 0 else \n",
    "                                         ('Bas' if x <= 2 else \n",
    "                                         ('Moyen' if x <= 5 else 'Haut')))\n",
    "\n",
    "# 7. Ratio rooms/surface (densit√©)\n",
    "df['room_density'] = df['rooms'] / df['surface_m2']\n",
    "\n",
    "# 8. Transformation log de la surface (pour lin√©ariser la relation)\n",
    "df['log_surface'] = np.log1p(df['surface_m2'])\n",
    "\n",
    "# 9. Interaction ville √ó surface\n",
    "df['city_surface_interaction'] = df['city'] + '_' + df['surface_category'].astype(str)\n",
    "\n",
    "# 10. Feature polynomiale pour surface (capturer non-lin√©arit√©)\n",
    "df['surface_squared'] = df['surface_m2'] ** 2\n",
    "\n",
    "print(f\"\\n‚úÖ {10} nouvelles features cr√©√©es\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nNouvelles colonnes:\")\n",
    "new_cols = ['price_per_m2', 'surface_per_room', 'age_category', 'surface_category', \n",
    "            'comfort_score', 'floor_category', 'room_density', 'log_surface',\n",
    "            'city_surface_interaction', 'surface_squared']\n",
    "for col in new_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'impact des nouvelles features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Prix par cat√©gorie d'√¢ge\n",
    "df.boxplot(column='price', by='age_category', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Prix par Cat√©gorie d\\'√Çge', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Cat√©gorie d\\'√Çge')\n",
    "axes[0, 0].set_ylabel('Prix (‚Ç¨)')\n",
    "\n",
    "# Prix par cat√©gorie de surface\n",
    "df.boxplot(column='price', by='surface_category', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Prix par Cat√©gorie de Surface', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Cat√©gorie de Surface')\n",
    "axes[0, 1].set_ylabel('Prix (‚Ç¨)')\n",
    "\n",
    "# Prix par score de confort\n",
    "df.boxplot(column='price', by='comfort_score', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Prix par Score de Confort', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Score de Confort (0-3)')\n",
    "axes[1, 0].set_ylabel('Prix (‚Ç¨)')\n",
    "\n",
    "# Log(surface) vs Prix\n",
    "axes[1, 1].scatter(df['log_surface'], df['price'], alpha=0.5, s=10)\n",
    "axes[1, 1].set_title('Prix vs Log(Surface)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Log(Surface)')\n",
    "axes[1, 1].set_ylabel('Prix (‚Ç¨)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ENCODAGE DES VARIABLES CAT√âGORIELLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Copie pour l'encodage\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# 1. One-Hot Encoding pour 'city' (cardinal moyen)\n",
    "city_dummies = pd.get_dummies(df_encoded['city'], prefix='city', drop_first=True)\n",
    "df_encoded = pd.concat([df_encoded, city_dummies], axis=1)\n",
    "print(f\"\\n‚úÖ One-Hot Encoding: city ‚Üí {len(city_dummies.columns)} nouvelles colonnes\")\n",
    "\n",
    "# 2. Ordinal Encoding pour 'energy_class' (ordre naturel: A > B > ... > G)\n",
    "energy_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\n",
    "df_encoded['energy_class_encoded'] = df_encoded['energy_class'].map(energy_mapping)\n",
    "print(f\"‚úÖ Ordinal Encoding: energy_class ‚Üí energy_class_encoded\")\n",
    "\n",
    "# 3. One-Hot Encoding pour les nouvelles cat√©gories\n",
    "for col in ['age_category', 'surface_category', 'floor_category']:\n",
    "    dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=True)\n",
    "    df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "    print(f\"‚úÖ One-Hot Encoding: {col} ‚Üí {len(dummies.columns)} nouvelles colonnes\")\n",
    "\n",
    "# 4. Target Encoding pour 'city_surface_interaction' (haute cardinalit√©)\n",
    "# Calculer la moyenne du prix par groupe\n",
    "target_encoding = df_encoded.groupby('city_surface_interaction')['price'].mean().to_dict()\n",
    "df_encoded['city_surface_encoded'] = df_encoded['city_surface_interaction'].map(target_encoding)\n",
    "print(f\"‚úÖ Target Encoding: city_surface_interaction ‚Üí city_surface_encoded\")\n",
    "\n",
    "print(f\"\\nShape apr√®s encodage: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner les features finales pour le mod√®le\n",
    "# Exclure les colonnes originales et les colonnes non n√©cessaires\n",
    "cols_to_drop = ['city', 'energy_class', 'age_category', 'surface_category', \n",
    "                'floor_category', 'city_surface_interaction', 'price_per_m2']\n",
    "\n",
    "df_model = df_encoded.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURES FINALES POUR LE MOD√àLE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nNombre total de features: {df_model.shape[1] - 1}\")\n",
    "print(f\"\\nListe des features:\")\n",
    "feature_cols = [col for col in df_model.columns if col != 'price']\n",
    "for idx, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {idx}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"NORMALISATION DES FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# S√©parer X et y\n",
    "X = df_model.drop('price', axis=1)\n",
    "y = df_model['price']\n",
    "\n",
    "# Identifier les colonnes num√©riques √† scaler\n",
    "# (exclure les colonnes one-hot encod√©es qui sont d√©j√† 0/1)\n",
    "numeric_cols = ['surface_m2', 'rooms', 'age_years', 'floor', 'comfort_score',\n",
    "                'surface_per_room', 'room_density', 'log_surface', 'surface_squared',\n",
    "                'city_surface_encoded', 'energy_class_encoded']\n",
    "\n",
    "# Utiliser RobustScaler (r√©sistant aux outliers)\n",
    "scaler = RobustScaler()\n",
    "X_scaled = X.copy()\n",
    "\n",
    "# Scaler uniquement les colonnes num√©riques qui existent\n",
    "cols_to_scale = [col for col in numeric_cols if col in X.columns]\n",
    "X_scaled[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
    "\n",
    "print(f\"\\n‚úÖ {len(cols_to_scale)} features normalis√©es avec RobustScaler\")\n",
    "print(f\"\\nFeatures scal√©es:\")\n",
    "for col in cols_to_scale:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Sauvegarder le scaler pour la production\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "print(\"\\n‚úÖ Scaler sauvegard√©: ../models/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la transformation log sur la cible\n",
    "print(\"=\" * 80)\n",
    "print(\"TRANSFORMATION DE LA CIBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Statistiques avant transformation\n",
    "print(\"\\nAvant transformation:\")\n",
    "print(f\"  Skewness: {y.skew():.3f}\")\n",
    "print(f\"  Kurtosis: {y.kurtosis():.3f}\")\n",
    "\n",
    "# Transformation log\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "print(\"\\nApr√®s transformation log:\")\n",
    "print(f\"  Skewness: {y_log.skew():.3f}\")\n",
    "print(f\"  Kurtosis: {y_log.kurtosis():.3f}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Distribution originale\n",
    "axes[0].hist(y, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(f'Distribution Originale (Skew: {y.skew():.2f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Prix (‚Ç¨)')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "\n",
    "# Distribution log\n",
    "axes[1].hist(y_log, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_title(f'Distribution Log-Transform√©e (Skew: {y_log.skew():.2f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Log(Prix)')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° La transformation log r√©duit le skewness et rend la distribution plus normale\")\n",
    "print(\"   ‚Üí Utile pour les mod√®les lin√©aires (LinearRegression, Ridge, Lasso)\")\n",
    "print(\"   ‚Üí Moins crucial pour les tree-based models (RandomForest, XGBoost)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de corr√©lation pour feature selection\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE SELECTION - ANALYSE DE CORR√âLATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculer corr√©lation avec la cible\n",
    "correlations = X_scaled.corrwith(y).abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 features les plus corr√©l√©es avec le prix:\")\n",
    "print(correlations.head(15))\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "correlations.head(20).plot(kind='barh', ax=ax, edgecolor='black', alpha=0.7)\n",
    "ax.set_title('Top 20 Features - Corr√©lation avec le Prix', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Corr√©lation Absolue')\n",
    "ax.set_ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identifier les features √† faible corr√©lation (potentiellement √† supprimer)\n",
    "low_corr_features = correlations[correlations < 0.05]\n",
    "print(f\"\\n‚ö†Ô∏è  {len(low_corr_features)} features avec corr√©lation < 0.05:\")\n",
    "if len(low_corr_features) > 0:\n",
    "    print(low_corr_features)\n",
    "    print(\"\\nüí° Consid√©rer de retirer ces features pour simplifier le mod√®le\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation entre features (d√©tecter multicollin√©arit√©)\n",
    "corr_matrix = X_scaled.corr().abs()\n",
    "\n",
    "# Trouver les paires de features tr√®s corr√©l√©es (> 0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] > 0.9:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i], \n",
    "                corr_matrix.columns[j], \n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚ö†Ô∏è  FEATURES HAUTEMENT CORR√âL√âES (Multicollin√©arit√©)\")\n",
    "    print(\"=\" * 80)\n",
    "    for feat1, feat2, corr_val in high_corr_pairs:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr_val:.3f}\")\n",
    "    print(\"\\nüí° Consid√©rer de supprimer l'une des deux features dans chaque paire\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Pas de multicollin√©arit√© d√©tect√©e (corr√©lation > 0.9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SPLIT TRAIN/TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split 80/20 avec stratification par ville (si possible)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Aussi cr√©er les versions log-transform√©es de y\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Split effectu√©:\")\n",
    "print(f\"  Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"  Test set:  {X_test.shape[0]} samples ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"  Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Statistiques\n",
    "print(f\"\\nStatistiques Train set:\")\n",
    "print(f\"  Prix moyen: {y_train.mean():,.2f} ‚Ç¨\")\n",
    "print(f\"  Prix m√©dian: {y_train.median():,.2f} ‚Ç¨\")\n",
    "print(f\"  √âcart-type: {y_train.std():,.2f} ‚Ç¨\")\n",
    "\n",
    "print(f\"\\nStatistiques Test set:\")\n",
    "print(f\"  Prix moyen: {y_test.mean():,.2f} ‚Ç¨\")\n",
    "print(f\"  Prix m√©dian: {y_test.median():,.2f} ‚Ç¨\")\n",
    "print(f\"  √âcart-type: {y_test.std():,.2f} ‚Ç¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAUVEGARDE DES DONN√âES PR√âPAR√âES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sauvegarder les datasets\n",
    "import pickle\n",
    "\n",
    "# Cr√©er un dictionnaire avec tous les datasets\n",
    "data_dict = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'y_train_log': y_train_log,\n",
    "    'y_test_log': y_test_log,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'scaler': scaler\n",
    "}\n",
    "\n",
    "# Sauvegarder\n",
    "with open('../data/processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_dict, f)\n",
    "\n",
    "print(\"\\n‚úÖ Donn√©es sauvegard√©es: ../data/processed_data.pkl\")\n",
    "print(\"\\nContenu du fichier:\")\n",
    "print(\"  - X_train, X_test (features normalis√©es)\")\n",
    "print(\"  - y_train, y_test (cible originale)\")\n",
    "print(\"  - y_train_log, y_test_log (cible transform√©e)\")\n",
    "print(\"  - feature_names (liste des features)\")\n",
    "print(\"  - scaler (pour la production)\")\n",
    "\n",
    "# Sauvegarder aussi en CSV pour inspection\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "train_df.to_csv('../data/train_processed.csv', index=False)\n",
    "test_df.to_csv('../data/test_processed.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ √âgalement sauvegard√© en CSV:\")\n",
    "print(\"  - ../data/train_processed.csv\")\n",
    "print(\"  - ../data/test_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Features cr√©√©es:\n",
    "1. **Numerical transformations:**\n",
    "   - `price_per_m2`: Prix au m¬≤\n",
    "   - `surface_per_room`: Surface par pi√®ce\n",
    "   - `log_surface`: Transformation log de la surface\n",
    "   - `surface_squared`: Surface au carr√© (polynomial)\n",
    "   - `room_density`: Densit√© (rooms/surface)\n",
    "\n",
    "2. **Categorical features:**\n",
    "   - `age_category`: Cat√©gories d'√¢ge (Neuf/R√©cent/Moyen/Ancien)\n",
    "   - `surface_category`: Cat√©gories de surface (Studio/Moyen/Grand/Tr√®s_grand)\n",
    "   - `floor_category`: Cat√©gories d'√©tage (RDC/Bas/Moyen/Haut)\n",
    "   - `comfort_score`: Score 0-3 (elevator + parking + balcony)\n",
    "\n",
    "3. **Interactions:**\n",
    "   - `city_surface_encoded`: Interaction ville √ó surface (target encoded)\n",
    "\n",
    "### Encodings appliqu√©s:\n",
    "- One-Hot Encoding: city, age_category, surface_category, floor_category\n",
    "- Ordinal Encoding: energy_class (A=7 ... G=1)\n",
    "- Target Encoding: city_surface_interaction\n",
    "\n",
    "### Preprocessing:\n",
    "- Outliers: capp√©s avec m√©thode IQR\n",
    "- Scaling: RobustScaler sur features num√©riques\n",
    "- Target: log transformation disponible (optionnel)\n",
    "\n",
    "### Next Step:\n",
    "**Notebook 03 - Model Training**: Entra√Æner et comparer diff√©rents mod√®les ML\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
